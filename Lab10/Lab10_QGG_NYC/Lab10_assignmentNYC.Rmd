---
title: "BTRY4830_Lab10"
author: "Scott Kulm"
date: "4/11/2019"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=FALSE, echo = TRUE)
library(ggplot2)
library(MASS)
```


## 0. Overview

1. Reading in Data

\hspace*{0.25in} Review of data types

\hspace*{0.25in} What to do when bad data strikes

2. Calculate p-value with covariates

\hspace*{0.25in} Compare F-statitstic calculation with and without covariates

\hspace*{0.25in} Simulate some phenotype data with covariates

3. Logistic Regression

\hspace*{0.25in} GWAS model for catgorical phenotypes (binary, take values 0 or 1)

4. Exercise--Implementing the IRLS Algorithm to conduct a Logistic Regression GWAS


## 1. Reading in Data

We have been reading in data from genotype and phenotype files for the past few labs, but now let's slow down and actualy discuss the different arguments that can help with this process.  The basic arguments are header which is a boolean argument deciding whether the first row should become the rownames, and row.names which is a numeric or F 

```{r}
geno <- read.table("example.tsv", header = T, row.names = 1)
```

In order to remove these row names set the argument to null.  If the length of the header line was the same as the first row we could set header = F to also remove the column names, but because the header line is shorter we cannot.  You can test this out.

```{r}
geno <- read.table("example.tsv", header = T, row.names = NULL)
head(geno)
```

However this process would not work if you tried to read in a comma separated file:

```{r}
geno <- read.table("example.csv", header = T, row.names = 1)
head(geno)
```

We have to specify the delimiter or switch to using the read.csv() function:

```{r}
geno <- read.table("example.csv", header = T, row.names = 1, sep=",")
geno <- read.csv("example.csv", header = T, row.names = 1)
head(geno)
```

Look at the data types of each column, they are factors.  Factors are a data type, just like numeric, character, or integers.  There are two parts of a factor, the data and the levels.  To a computer the data is a basic integer 1,2,3, ... and the levels are the translation 1="a", 2="b", 3="c", ... .  

```{r}
str(geno[,1])
```

Factors can be great for two reasons: you have catagorical data or you have lots of data.

```{r}
x <- sample(c("AA","CC"),10000,replace = T)
object.size(x)
y <- as.factor(x)
object.size(y)
```

```{r}
df <- data.frame(x=1:100,y=sort(rnorm(100)),
                 category=c(rep(1,50),rep(2,50)))

ggplot(df,aes(x,y))+geom_point(aes(color=category))

df$category <- as.factor(df$category)
ggplot(df,aes(x,y))+geom_point(aes(color=category))
```

Factors can also come with plenty of problems, as you cannot do any calculations with factors and they do not sort as you would expect:

```{r}
x <- as.factor(1:10)
mean(x)
```

Therefore, it is likely a good idea to just steer clear of factors unless we know we want them for a particular purpose.  To do this we simply set stringsAsFactors=F.

```{r}
geno <- read.csv("example.csv", header = T, row.names = 1, stringsAsFactors = F)
head(geno)
```

There are plenty of other useful read.table arguments, and even other functions altogether.  However what I would consider to be the 4 major arguments of read.table have just been covered.  If messing with these arguments is still not getting the data into R, try to open the data file in vi, emacs, NotePad, or textEdit.  Check out the delimiter, does it change throughout the data, or those spaces or are there tabs, do some lines have more elements than others?  Try to tidy up the data so each row has the same number of elements accoring to one consistent delimiter and the data should be able to be read into R.  Further command line tools such as head, tail, cut, and tr can also be very helpful.


## 1. Covariates

Recall in Lab 7 that the p-value calculator used an F-statistic ratio that was calculated using the variance between the estimates and the mean along with the variance between the estimates and the real phenotype values.  We have seen this in lecture as:

\begin{equation}
SSM = \sum^n_{i=1} ( \hat{y_i} - \bar{y} )^2 \;\;\;\;\;\;\;\;\;\; SSE = \sum^n_{i=1} ( y_i - \hat{y_i} )^2 \;\;\;\;\;\;\;\;\;\; F_{[2,n-3]} \;=\;  \frac{\;\;\;\;\frac{SSM}{df(M)}\;\;\;\;}{\frac{SSE}{df(E)}} \;=\; \frac{\;\;\;\;\frac{SSM}{2}\;\;\;\;}{\frac{SSE}{n-3}}
\end{equation}


And in previous labs within the pval_calculator:


```{r lab7pvalcalculator}
# Lab 7 code calculating Fstatistic according to Lecture 12 equations

# Function to calculate the pval given a set of individuals' phenotype, and genotype encodings.
pval_calculator_lab7 <- function(pheno_input, xa_input, xd_input){
    n_samples <- length(xa_input)
    
    X_mx <- cbind(1,xa_input,xd_input)
    
    MLE_beta <- ginv(t(X_mx) %*% X_mx) %*% t(X_mx) %*% pheno_input
    y_hat <- X_mx %*% MLE_beta
  
    SSM <- sum((y_hat - mean(pheno_input))^2)
    SSE <- sum((pheno_input - y_hat)^2)
  
    df_M <- 2
    df_E <- n_samples - 3 
  
    MSM <- SSM / df_M
    MSE <- SSE / df_E
    
    Fstatistic <- MSM / MSE
  
    pval <- pf(Fstatistic, df_M, df_E,lower.tail = FALSE)
    
    return(pval)
}
```

We have since learned and shown with the lm() function that it is often important to model covariates that might be skewing our data.  While the lm() function is a quick covariate method, it is important to understand what is going on "under the hood".

Below is a function for calculating the F-statistic based on the equations from Lecture 15. It is important to remember that this pvalue is controlling for covariates by including the covariates in the null hypothesis.

By looking at the equation below we see that the process is nearly exactly the same as above, all we are doing is changing the form of the F-statistic slightly.


\begin{equation}
SSE(\hat{\theta_0}) = \sum^n_{i=1} ( y_i - \hat{y}_{i,\hat{\theta_0}} )^2 \;\;\;\;\;\;\;\;\;\; SSE(\hat{\theta_1}) = \sum^n_{i=1} ( y_i - \hat{y}_{i,\hat{\theta_1}} )^2 \;\;\;\;\;\;\;\;\;\; F_{[2,n-3]} \;=\; \frac{\;\;\;\;\frac{SSE(\hat{\theta_0})-SSE(\hat{\theta_1})}{2}\;\;\;\;}{\frac{SSE(\hat{\theta_1})}{n-3}}
\end{equation}

```{r lab10pvalcalculator}
# New function to calculate the pval given a set of individuals' phenotype, and genotype encodings, adjusting null to include covariates.
pval_calculator_lab10 <- function(pheno_input, xa_input, xd_input, z_input){
    n_samples <- length(xa_input)
    
    # Set up random variables for null (Z_mx) and with genotypes (XZ_mx)
    Z_mx <- cbind(1,z_input)                                          # H0 (w/ covariates)
    XZ_mx <- cbind(1,xa_input,xd_input,z_input)                       # w/ genotypes too
    
    # Calculate MLE betas for both null model and model with genotypes and covariates
    MLE_beta_theta0 <- ginv(t(Z_mx)  %*% Z_mx)  %*% t(Z_mx)  %*% pheno_input 
    MLE_beta_theta1 <- ginv(t(XZ_mx) %*% XZ_mx) %*% t(XZ_mx) %*% pheno_input
    
    # Get Y estimates using the betas calculated above to give each hypothesis its best chance
    y_hat_theta0 <- Z_mx  %*% MLE_beta_theta0
    y_hat_theta1 <- XZ_mx %*% MLE_beta_theta1
    
    # Get the variance between the true phenotype values and our estimates under each hypothesis
    SSE_theta0 <- sum((pheno_input - y_hat_theta0)^2)
    SSE_theta1 <- sum((pheno_input - y_hat_theta1)^2)
    
    # Set degrees of freedom
    df_M <- 2
    df_E <- n_samples - 3
    
    # Put together calculated terms to get Fstatistic
    Fstatistic <- ((SSE_theta0-SSE_theta1)/df_M) / (SSE_theta1/df_E)
    
    # Determine pval of the Fstatistic
    pval <- pf(Fstatistic, df_M, df_E,lower.tail = FALSE)
    return(pval)
}
```

This code looks great, but that's no proof it works. Let's try it out with actual data where we generate a bunch of genotypes as Xa and Xd encodings, and then generate the phenotypes based on these encodings.

First comes the encodings and a covariate, each of which are independent:

```{r simulateData}
set.seed(2019)

# Set the dimensions of the data (Number of individuals and polymorphic sites to obtain genotypes for)
n_individuals = 1500
n_polymorphic_sites = 1000
# simulate genotypes as Xa and Xd encodings using HW frequencies with each allele freq=0.5
xa_sim <- matrix( sample(c(1,0,-1),
                         n_individuals*n_polymorphic_sites,
                         replace=T,
                         prob=c(0.25, 0.5, 0.25)), 
                  nrow = n_individuals,
                  ncol = n_polymorphic_sites)
xd_sim <- 2*abs(xa_sim) -1
# simulate two covariates
z_sim <- cbind( sample(c(0, 1), n_individuals, replace=T, prob = c(0.5,0.5)),
                sample(c(0, 1), n_individuals, replace=T, prob = c(0.5,0.5)) )
```

Second we pick the index of the causal site, the betas that go into an equation that produces the phenotypes.  We must also include some random error in the phenotypes. 

```{r}
# simulate phenotypes based on the following true parameters
causal_site <- 150
true_betas <- c(  0.3,  0.2, -0.1,  0.9,  0.7 )  # Bmu, Ba, Bd, Bz1, Bz2, ...
epsion_sigmasq <- 1                  # sigma_sq in \epsilon=N(0,sigma_sq)
pheno <- cbind( 1, xa_sim[,causal_site], xd_sim[,causal_site], z_sim ) %*% true_betas + rnorm(n_individuals, sd = sqrt(epsion_sigmasq))
```


Below we used the simulated data to calculate pvalues for each polymorphic site using the calculators from lab7 and the new one we are introducing in this lab now. 


```{r}
# Initialize some variables and constants
pval_mx <- matrix(NA, nrow = n_polymorphic_sites, ncol=2)

# Calculate and save pvals for each phenotype-genotype pair
for (i in 1 : n_polymorphic_sites){
  pval_mx[i,1] <- pval_calculator_lab7(pheno_input = pheno,
                                       xa_input = xa_sim[,i],
                                       xd_input = xd_sim[,i])
  pval_mx[i,2] <- pval_calculator_lab10(pheno_input = pheno,
                                        xa_input = xa_sim[,i],
                                        xd_input = xd_sim[,i],
                                        z_input = z_sim)
}
```


We can then compare the QQ-plots side-by-side.  You can see that controlling for covariates using the new pvalue calculator tightens the data around the y=x red line, where out pvalues ideally would reside.


```{r}
par(mfrow=c(1,2))

# Compare QQ plots
expected_vals <- seq(1/n_polymorphic_sites, 1, by=1/n_polymorphic_sites)
observed_lab7 <- sort(pval_mx[,1])
observed_lab10<- sort(pval_mx[,2])

plot(-log10(expected_vals), -log10(observed_lab7),
     ylim=c(0,5))
abline(a=0,b=1,col='red')

plot(-log10(expected_vals), -log10(observed_lab10),
     ylim=c(0,5))
abline(a=0,b=1,col='red')
```

And finally, from the manhattan plot of the new pvalue calculator, we can identify the correct causal site.

```{r}
par(mfrow=c(1,1))
plot(-log10(pval_mx[,2]) )
abline(h=-log10(0.05/n_individuals), col='red')
```



## 2. Logistic Regression

Next we will be talking about **logistic regression.** In a logistic regression, the dependent variable y (in our case phenotype) is **categorical** instead of continuous. Specifically, we are going to use logistic regression to deal with binary phenotypes coded as 0 and 1. For example, in genome-wide association studies (GWAS) a healthy or normal control phenotype would be 0 and a disease phenotype (ex. diabetes, alzheimers, etc ...) would be 1, and the goal is to identify genomic variations that increase the probability of belonging to the disease category. 

You might be wondering why we need this in the first place. So let's try to use linear regression for a binary phenotype and see what happens. First we need to generate some data, similar to in the lecture we'll look at one polymorphism.  This polymorphism can be considered a minor allele, as most of the genotypes are coded as 0.  


```{r}
xa <- c(runif(275,min=0,max=50), runif(45,50,100))
y <- round(xa/100)
xa <- c(rep(-1,1640),rep(0,0),rep(1,8),
        rep(-1,1640),rep(0,0),rep(1,1))
y <- c(rep(0,1648), rep(1,1641))

test_data <- data.frame("pheno" = y, "Xa" = xa)
ggplot(test_data,aes(Xa,pheno))+geom_jitter(width=0.1,height=0.1)+geom_smooth(method=lm)
```


You have been previously been taught why it is important to be wary of low minor alleles, which provides a good testing ground for our two forms of regression.  Let's calculate the signifcance of genotype to phenotype using each model: 


```{r ,echo = FALSE,fig.show='hold',  fig.width=4, fig.height=4, fig.align='center'}
linearModel <- lm(pheno ~ Xa, data=test_data)
cat("The PVal is", summary(linearModel)$coefficients[2,4]," for linear regression \n")

logisticModel <- glm(pheno ~ Xa, data=test_data, family="binomial")
cat("The PVal is", summary(logisticModel)$coefficients[2,4]," for logistic regression")
```

Here we can clearly see how the logistic regression does not get fooled into drawing too much significance from the outlying points whereas the linear regression does, as the polymorphism is significant under the linear regrssion and insignificant under logistic regression.  

How does logistic regression exactly work? Simply put, logistic regression transforms the linear model that we use to "fit" the binary phenotypes. The logistic function takes the form as follows:

$$\sigma(t)=\frac{1}{1+e^{-t}}$$

So if we substitute t with the linear function that depends on x and beta values, the logistic function that we use becomes

$$F(x)=\frac{1}{1+e^{-(\beta_0+\beta_1x)}}$$


A simple visualization in R might give us a better idea of how the data is transformed. Basically, logistic regression confines the original dependent values within the range of 0 and 1. 

```{r, echo = TRUE, fig.show='hold',  fig.width=8, fig.height=4, fig.align='center' }
x <- seq(-1,1,by = 0.1)
y_linear <- x * 4
y_logistic <- 1 / ( 1 + exp(-y_linear))

par(mfrow = c(1,2))
plot(x, y_linear, main = "Linear function", type = "l")
abline(h = 0, col = "red", lty = 2)
abline(h = 1, col = "red", lty = 2) 
plot(x,y_logistic, main = "logistic regression curve", type ="l", ylim = c(-0.5,1.5))
abline(h = 0, col = "red", lty = 2)
abline(h = 1, col = "red", lty = 2)
```

Let's project the settings in our problem onto the above equation and clarify our goal. We have a given phenotype that takes either a value of 1 or 0, and two matrices for genotypes in the form of Xa(-1,0,1) coding and Xd(-1,1) coding. Just like in linear regression, the goal is to find the values for $\beta_{\mu}$, $\beta_a$ and $\beta_d$ for each genotype that best explain the relationship between the genotype and phenotype. 

If the relationship was error free and the genotype value directly predicts the phenotype, we would not need logistic regression (For example, if A2A2 indicates phenotype = 1 with 100% certainty). However, that is more than often not the case in real world genetics/genomics so we would have to "soft" model the relationship between genotypes and phenotypes by using probabilities, (In other words, A2A2 has a higher chance of having phenotype=1 than phenotype=0) and that is what the transformation given in the above equation is doing. 


## 3. IRLS Algoritm

As you have been going over in lecture the IRLS algorithm stands for iterated reweighted least squares - sounds fancy! But in fact it comes from an algorithm that has been around for hundreds of years - Newton's method!  The basics of Newton's method is that we want to find the place where a curvy line intersects the x-axis.  We could just guess, but it becomes much faster to use the slope of the line to help us form the prediction.

```{r, out.width = "400px"}
knitr::include_graphics("newtons-method.png")
```

This is the same exact process that we will follow, except instead of the slope of the line we are trying to find to values of the betas for Xa and Xd.  And since we are looking at a sigmoid, the updating equation (similar to finding the tangential line above) is slightly more complex.  With these similarities in mind we can break down the IRLS algorithm for logistic regression into:

  1. With a set of betas, calculate W, which is similar to our guess of y
  2. Use this W to then update the betas, similar to finding the slope of the line
  3. Check if these betas are similar to the previous betas, which is really measuring the deviance

### Exercise

It's time to implement the IRLS algorithm!  To make this "real" we will be looking at some real data, and to make this easy we will break the algorithm down into easy to interpret steps.

1) Download the phenotype and genotype and read them in.
   You should have 292 genotypes and 1 phenotype for 107 samples.
   
2) Note that the genotypes are already in Xa codings, and you only have to create the Xd matrix from it.

3) Use the template given below and try to fill in the code to make it a functional algorithm.

4) Plot a manhattan plot for the phenotype and look for significant peaks.

5) Your output should look something like shown below.

This is obviously a big task, so please try to just fill in some of the basic functions by translating the equations in the slides.  If you are not able to calculate pvalues that is ok.

```{r, comment = NA, echo = TRUE,eval = FALSE, fig.align='center', fig.width=7,fig.height=4}
W_calc <- function(gamma_inv){
     
    return(W)
}


beta_update <- function(X_mx, W, Y, gamma_inv, beta){
	return(beta_up)
}


gamma_inv_calc <- function(X_mx, beta_t){
    return(gamma_inv)
}


dev_calc <- function(Y, gamma_inv){
    return(deviance)
}


loglik_calc <- function(Y, gamma_inv){
    return(loglik)
}


logistic.IRLS<- function(Xa,Xd,Y = Y, beta.initial.vec = c(0,0,0), d.stop.th = 1e-6, it.max = 100) {
    
    # Create Initial values
    
  
    # Start of optimization loop
    for(i in 1:it.max) {
         
        # calculate W
      
        # update beta
      
        # update gamma_inv
      
        # calculate deviation
      
        # check if deviation is smaller than threshold
        if() {
            cat("Convergence at iteration:", i, "at threshold:", d.stop.th, "\n")
            logl<-# Log likelihood goes here
            return(list(beta_t,logl)) # return a list that has beta.t and logl saved
        }   
    }
  
    # In case the algorithm did not coverge
    cat("Convergence not reached after iteration:", i, "at threshold:", d.stop.th, "\n")
    return(list(beta_t= c(NA,NA,NA),logl=NA)) # return NA values 
}

G <- dim(Xa)[2]
logl <- vector(length = G)
for(j in 1:G){
  result.list <- call our function
  logl<- # How do we extract an element from a list? might want to use [[]]
}
# Calculate the log likelihood for the NULL using IRLS
logl_H0 <- logistic.IRLS(Y=Y, Xa= NULL, Xd=NULL, beta.initial.vec = c(0))[[2]]
LRT<-2*logl-2*logl_H0 #likelihood ratio test statistic
pval <- # chi squared test with the following parameters (LRT, 2, lower.tail = F)
  
# Plot manhattan plot with cut off line  
plot(-log10(pval))
abline(-log10(0.05/300),0,col="red")
```


